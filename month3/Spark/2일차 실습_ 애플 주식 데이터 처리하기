{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"colab":{"provenance":[{"file_id":"1toRN7fs_OFYj4DYWQ40Tqt0W4ON6pafl","timestamp":1686635537030}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"PYwHJBmiz6e5"},"source":["애플 주식 데이터를 가지고 간단한 데이터 분석을 해보자. 모든 답은 Pyspark을 통해 이뤄져야 한다."]},{"cell_type":"markdown","metadata":{"id":"SE0VhL0g1no8"},"source":["먼저 PySpark과 Py4J를 설치하자"]},{"cell_type":"code","metadata":{"id":"YXgIyS_F0Kar","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686637351303,"user_tz":-540,"elapsed":40699,"user":{"displayName":"문건희","userId":"18327497055746524936"}},"outputId":"2eeaded7-1776-4d4b-f90b-00108b87731b"},"source":["!pip install pyspark==3.0.1 py4j==0.10.9"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark==3.0.1\n","  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.2/204.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9\n","  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612229 sha256=47df24a1a62128a6baf9dbda1ff51848a44b6e7f2252f85cdea757dbb8f99109\n","  Stored in directory: /root/.cache/pip/wheels/19/b0/c8/6cb894117070e130fc44352c2a13f15b6c27e440d04a84fb48\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","  Attempting uninstall: py4j\n","    Found existing installation: py4j 0.10.9.7\n","    Uninstalling py4j-0.10.9.7:\n","      Successfully uninstalled py4j-0.10.9.7\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"FNwc3F_Az6e6"},"source":["#### Spark Session 만들기"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"RveyavjYz6e7","executionInfo":{"status":"ok","timestamp":1686637362071,"user_tz":-540,"elapsed":10785,"user":{"displayName":"문건희","userId":"18327497055746524936"}}},"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark Dataframe basic example\") \\\n","    .getOrCreate()"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","name_list_json = [\n","    '{\"name\": \"keeyoung\"}',\n","    '{\"name\": \"benjamin\"}',\n","    '{\"name\": \"claire\"}'\n","]\n","\n","for n in name_list_json:\n","    js = json.loads(n)\n","    print(js['name'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NisowEY6au86","executionInfo":{"status":"ok","timestamp":1686637413349,"user_tz":-540,"elapsed":12,"user":{"displayName":"문건희","userId":"18327497055746524936"}},"outputId":"a6163273-e9f2-472c-a905-b5c690ed3f55"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["keeyoung\n","benjamin\n","claire\n"]}]},{"cell_type":"markdown","metadata":{"id":"b0DgR89Sz6e8"},"source":["#### 애플 주식 CSV 파일 로딩하기: https://pyspark-test-sj.s3-us-west-2.amazonaws.com/appl_stock.csv\n","일단 pandas 데이터프레임으로 로딩해서 Spark 데이터프레임으로 변경한다"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"FeKmU3Piz6e8"},"source":["import pandas as pd\n","\n","apple_pandas_df = pd.read_csv(\"https://pyspark-test-sj.s3-us-west-2.amazonaws.com/appl_stock.csv\")\n","apple_spark_df = spark.createDataFrame(apple_pandas_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NA3fbJ4Lz6e9"},"source":["#### 1> 어떤 컬럼 이름들이 있는가?"]},{"cell_type":"code","metadata":{"id":"b341_1Zfz6e9"},"source":["apple_spark_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uXj2LCWuz6e_"},"source":["#### 2> 스키마를 프린트해보기"]},{"cell_type":"code","metadata":{"id":"NQR5dwZjz6e_"},"source":["apple_spark_df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFljhmp5z6fA"},"source":["#### 3> 처음 5개의 레코드를 출력해보기"]},{"cell_type":"code","metadata":{"id":"OJQZ7PZDz6fA"},"source":["apple_spark_df.show(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-uZPeXHxz6fB"},"source":["#### 4> describe를 사용하여 데이터프레임의 컬럼별 통계보기"]},{"cell_type":"code","metadata":{"id":"8Apv3ZhW2Zhj"},"source":["apple_spark_df.describe().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eR1QL8-Z2auU"},"source":["#### 5> Close 컬럼의 평균값은 얼마인가?"]},{"cell_type":"code","metadata":{"id":"ZCQCMa0xz6fB"},"source":["from pyspark.sql.functions import mean\n","\n","apple_spark_df.select(mean(\"Close\")).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YnE6Cbg_IONn"},"source":["#### 6> Volume 컬럼의 최대값과 최소값은?"]},{"cell_type":"code","metadata":{"id":"I5mvFy0eIVPx"},"source":["from pyspark.sql.functions import min, max\n","\n","apple_spark_df.select(max(\"Volume\"), min(\"Volume\")).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ax1Of8ATz6fD"},"source":["#### 보너스 질문: HV ratio라는 이름의 새로운 컬럼을 추가한 데이터프레임을 만들기. 이 컬럼의 값은 High/Volume으로 계산된다"]},{"cell_type":"code","metadata":{"id":"MkO7rQ3Pz6fD"},"source":["apple_spark_df_with_hv = apple_spark_df.withColumn(\"hv ratio\", apple_spark_df.High/apple_spark_df.Volume)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ce_bSkvOHEDC"},"source":["apple_spark_df_with_hv.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BIROQ8klz6fD"},"source":["#### 보너스 질문: 월별 Close 컬럼의 평균값은?"]},{"cell_type":"code","metadata":{"id":"Q_mr0fO_z6fD"},"source":["from pyspark.sql.functions import month\n","\n","monthdf = apple_spark_df.withColumn(\"Month\", month(\"Date\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3mfqkYUHQ1e"},"source":["monthavgdf = monthdf.select([\"Month\", \"Close\"]).groupBy(\"Month\").mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eTpOy97YK_NC"},"source":["monthavgdf.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mjKxisALA2t"},"source":["monthavgdf.select([\"Month\", \"avg(Close)\"]).orderBy(\"Month\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5egHUUucLToO"},"source":[],"execution_count":null,"outputs":[]}]}